---
title: "Assignment One: The MAUP and Multilevel Modelling"
output:
  pdf_document:
    toc: true
    number_sections: true
bibliography: references.bib
header-includes:
   - \setlength\parindent{24pt}
   - \usepackage[table]{xcolor}
   - \usepackage{booktabs}
   - \usepackage{subfig}
---

\section{Demonstrating the MAUP}

\subsection{Background}

<!-- introduce problem -->

Areal units in zoning systems amalgamate into objects that constitute the basic units for the observation and analysis of spatial phenomena (Openshaw, 1984). Yet, no gold standard for guiding the spatial aggregation process exists, with the validity of zonal objects subject to the arbitrary and modifiable decision-making of quantitative geographers. Problematically, the analysis of socioeconomic data involving areal units is encumbered by the modifiable areal unit problem (MAUP): "the sensitivity of analytical results to the definition of units for which data are collected." According to the literature, the MAUP constrains the reliability of analyses for aggregated spatial data, as findings have shown varying results with the scale of aggregation and configuration of zoning systems (Clark and Avery, 1976).

In practice, the MAUP is condensed into two issues of scale and zoning sensitivity which this paper will attempt to demonstrate in Section \ref{maup}. The first issue, described as the *scale problem*, is the variation in findings when data for zonal units are progressively aggregated. This has been demonstrated empirically by Clark and Avery (1976) who found that whilst correlation coefficients did not increase monotonically with aggregation\footnote{At higher levels of aggregation, there is smaller adjacency of zonal units meaning groupings are more heterogenous leading to lower correlation coefficients.}, a general increase in data aggregation corresponds to an increase in correlation coefficients. 

The second issue, the *zoning problem*, pertains to the variation in findings when alternative combinations of zonal units are analysed with the scale or number of units held constant (Openshaw, 1984). Zoning sensitivity in multivariate analysis has been demonstrated empirically in Fotheringham and Wong (1991) who simulated the aggregation of 871 block groups into 218 zones in 150 different iterations. They highlight the severity of the zoning problem by demonstrating the possibility of concluding with one iteration of zones no association between the percentage of blue-collar workers and mean family income, with another iteration finding a unit increase in blue-collar worker percentages as reducing mean family income by \$20,000. In all, ignoring scale and zoning sensitivity in model calibration can lead to inferential conclusions that a researcher's areal data is applicable to the constituents who form the zones under study - the ecological fallacy problem (Openshaw, 1984).

\subsection{Demonstrating the MAUP}
\label{maup}

\subsubsection{Data}

To demonstrate the scaling and zoning sensitivities of the MAUP, we calculate the bivariate strength of association between two open data variables. For our first variable, *crime_count*, we submit a HTTP request using the POST verb to send a custom polygon for retrieving all street-level crimes occuring in 2012\footnote{For brevity, the code which binds the monthly dataframes together is moved to the Appendix.}. 

```{r dependencies, include=FALSE}
library(downloader)
library(rgdal)
library(httr)
library(ggplot2)
library(ggmap)
library(lattice)
library(sp)
library(maptools)
library(classInt)
library(RColorBrewer)
library(lme4)
library(merTools)
library(dplyr)
library(spatialEco)
library(cowplot)
```

```{r crime_data, eval=FALSE}
# download geojson
u <- "http://statistics.data.gov.uk/boundaries/E08000012.json"
# store in temporary directory 
downloader::download(url = u, destfile = "/tmp/lpool.geojson")
lpool <- readOGR(dsn = "/tmp/lpool.geojson", layer = "OGRGeoJSON")
# access coords slot
lpool <- lpool@polygons[[1]]@Polygons[[1]]@coords
# build lat/lon + date string to send with postrequest
curl.string <- paste0('poly=',paste0(sprintf('%s,%s',lpool[,2], lpool[,1])
                                     , collapse = ':'))

# build dates list for loop 
dates = c("2012-01", "2012-02", "2012-03", "2012-04", "2012-05", "2012-06",
          "2012-07", "2012-08", "2012-09", "2012-10", "2012-11", "2012-12")

document <- lapply(dates, function(month) {
  # format acceptable packet for http request
  curl.string <- list(poly=c(curl.string), date=c(month))
  # post custom polygon to police api 
  r <- httr::POST("https://data.police.uk/api/crimes-street/all-crime", 
                  body = curl.string, encode="multipart", verbose())
  json <- content(r, "text", encoding = "ISO-8859-1")
  # return as data.frame
  jsonlite::fromJSON(txt=json)
})
```

```{r, eval=FALSE}
# cast lat/lon columns to numeric data type
document$lat <- as.numeric(document$lat)
document$lon <- as.numeric(document$lon)

# convert data.frame to shapefile in rgdal
coordinates(document) <- ~lon+lat
proj4string(document) <- CRS("+init=epsg:4326") # WGS 84

# subsetting operations
document <- document[document$category == "violent-crime",]

# write once
writeOGR(document, "/Users/samcomber/Documents/spatial_analysis/shp/crimes",
         "crimes", driver = "ESRI Shapefile")

crimes <- readOGR(dsn = "/Users/samcomber/Documents/spatial_analysis/shp/crimes",
                  layer = "crimes")
crimes <- crimes[sample(nrow(crimes), 500), ]
```

Regarding our second variable, *tweet_count*, we aggregate geo-referenced tweets containing timestamps relating to Twitter postings within the municipality of Liverpool for 2012\footnote{This dataset was data mined by Guy Lansley from UCL, and processed by Dani Arribas-Bel.}.

```{r twitter_data, eval=FALSE}
# read twitter shapefile
tweets <- readOGR(dsn = "/Users/samcomber/Documents/spatial_analysis/shp/tweets",
                  layer = "tweets_mersey")

# sample 500 tweets
tweets <- tweets[sample(nrow(tweets), 500), ]
```
```{r, eval=FALSE,include=FALSE}
# alternate x and y for correct aggregation (oops)
lats <- tweets@coords[, 1]
lon <- tweets@coords[, 2]
tweets@coords[,1] <- lon
tweets@coords[,2] <- lats

tweets <- tweets[sample(nrow(tweets), 500), ]
```

To demonstrate the MAUP, we aggregate a count of each crime and tweet into separate variables for each region in the shapefile, before computing correlations between the two vectors of values in the dataframe. We investigate the scaling problem using an iterative process that increments the number of regions in a hex-binned lattice from 10 to 100 (stepping each iteration by 10 regions). To demonstrate the zoning problem we constrain cardinality - i.e. the number of regions - to 30, and calculate correlation coefficients using 10 different permutations of 30 regions. To build random regions, we use the multiple aggregatons feature in IMAGE studio (Konstatinos, 2017) which uses a depth-first search algorithm for a given contiguity matrix to create aggregated spatial regions. We then dissolve regions with the `unionSpatialPolygons` using the generated `ZoneID` for aggregations.

\subsubsection{Scale problem}

```{r, eval=FALSE}
# directory loop
folders <- dir("/Users/samcomber/Documents/spatial_analysis/shp/img_stud/run1/")

# we have folders + file names and are in run1 directory
setwd("/Users/samcomber/Documents/spatial_analysis/shp/img_stud/run1/")

# create master dataframe to hold correlation coefficents
cors.scale.master <- data.frame()

# loop over aggregation folder - i.e. 10 regions, 20, 30
for (folder in folders) {
  setwd(paste0("/Users/samcomber/Documents/spatial_analysis/shp/img_stud/run1","/"
               ,folder))
  
  # reread hex shapefile everytime we move folder
  hex <- readOGR(dsn = "/Users/samcomber/Documents/spatial_analysis/shp/hex", 
                 layer = "hex_1_clip")
  
  out <- data.frame()
  files <- list.files(pattern = "*.zon")
  
  # pick first iteration from each scale of aggregation
  img <- read.csv(files[1])

  # join regions to hex shapefile on area id
  hex@data$id <- as.numeric(hex@data$id)
  hex@data <- left_join(hex@data, img, by = c("id"= "AreaID"))
  hex@data <- hex@data[order(hex@data$id),]
  
  # dissolve by area id
  tes <- unionSpatialPolygons(hex, hex@data$ZoneID)

  # convert spatial polygons to spatialpolygonsdataframe
  oldw <- getOption("warn")
  
  # silence irrelevant warnings
  options(warn = -1)
  df <- data.frame(id = getSpPPolygonsIDSlots(tes))
  row.names(df) <- getSpPPolygonsIDSlots(tes)
  options(warn = oldw)
  
  spdf <- SpatialPolygonsDataFrame(tes, data = df)
  
  # points in polygon spatial join
  pts.crimes <- point.in.poly(crimes, spdf)
  pts.tweets <- point.in.poly(tweets, spdf)
  
  # aggregate crimes/tweets data
  pts.agg.crimes <- pts.crimes@data %>% group_by(id.1) %>% summarise(num = n()) 
  %>% arrange(id.1)
  pts.agg.tweets <- pts.tweets@data %>% group_by(id) %>% summarise(num = n()) 
  %>% arrange(id)
  
  # cast ids as factors for left_join
  pts.agg.crimes$id.1 <- as.integer(pts.agg.crimes$id.1)
  pts.agg.tweets$id <- as.integer(pts.agg.tweets$id)
  
  # join summed crimes/tweets back to data.frame
  hex@data <- left_join(hex@data, pts.agg.tweets, by = c("ZoneID" = "id"))
  hex@data <- left_join(hex@data, pts.agg.crimes, by = c("ZoneID" = "id.1"))
  
  # correlate unique zone id sums
  uniques <- hex@data[!duplicated(hex@data$num.x), ]
  cor.tc <- cor(uniques$num.x, uniques$num.y, use="complete.obs")
  
  # rbind to master frame
  cors.scale.master <- rbind(cors.scale.master, cor.tc)
}
```

\begin{figure}[!htbp]
\centering
\includegraphics{IMG/cowplot.jpeg}
\caption{Scale aggregations. A shows 10 regions, B shows 30 regions, C shows 60 regions and D shows 100 regions.}
\label{fig:scale}
\end{figure}

From \autoref{tab:scale}, we demonstrate the scaling problem. Generally, we observe rising correlations with increasing scale. Had the MAUP not existed, we would observe little systematic variation in the correlation between violent crime and tweets - we observe a difference of 0.36 between the upper and lower coefficients. Interestingly, we observe a correlation coefficient of 0.61 at 80 regions which lends evidence to Clark and Avery's (1976) findings that the low adjacency of zonal units at higher aggregations leads to lower correlation.

\begin{table}[!htbp]
\begin{center}
\caption{Correlations between crime and tweet counts.} \label{tab:scale}
\begin{tabular}{@{} *5l @{}}    \toprule
\emph{Number of regions} & \emph{Correlation coeff.} &&&  \\\midrule
 10  & 0.84  \\ 
 20  & 0.88 \\ 
 30  & 0.88 \\ 
 40  & 0.82 \\ 
 50  & 0.73 \\ 
 60  & 0.89 \\ 
 70  & 0.89 \\ 
 80  & 0.61 \\ 
 90  & 0.97 \\ 
 100 & 0.63 \\\bottomrule
 \hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{Zoning problem}

```{r, eval=FALSE}
# we choose 20 regions to illustrate zoning problem
setwd("/Users/samcomber/Documents/spatial_analysis/shp/img_stud/run1/z070")

# get all 10 iterations of 20 regions
files.20 <- list.files(pattern = "*.zon")
cors.zone.master
# build master data.frame
cors.zone.master <- data.frame()
for (file in files.20) {
  hex <- readOGR(dsn = "/Users/samcomber/Documents/spatial_analysis/shp/hex", 
                 layer = "hex_1_clip")
  
  # read file to df
  zon <- read.csv(file)
  
  # join regions to hex shapefile on area id
  hex@data$id <- as.numeric(hex@data$id)
  hex@data <- left_join(hex@data, zon, by = c("id"= "AreaID"))
  hex@data <- hex@data[order(hex@data$id),]
  
  # dissolve by area id
  tes <- unionSpatialPolygons(hex, hex@data$ZoneID)

  # convert spatial polygons to spatialpolygonsdataframe
  oldw <- getOption("warn")

  # silence irrelevant warnings
  options(warn = -1)
  df <- data.frame(id = getSpPPolygonsIDSlots(tes))
  row.names(df) <- getSpPPolygonsIDSlots(tes)
  options(warn = oldw)

  spdf <- SpatialPolygonsDataFrame(tes, data = df)

  # points in polygon spatial join
  pts.crimes <- point.in.poly(crimes, spdf)
  pts.tweets <- point.in.poly(tweets, spdf)

  # aggregate crimes/tweets data
  pts.agg.crimes <- pts.crimes@data %>% group_by(id.1) %>% summarise(num = n()) 
  %>% arrange(id.1)
  pts.agg.tweets <- pts.tweets@data %>% group_by(id) %>% summarise(num = n()) 
  %>% arrange(id)

  # cast ids as factors for left_join
  pts.agg.crimes$id.1 <- as.integer(pts.agg.crimes$id.1)
  pts.agg.tweets$id <- as.integer(pts.agg.tweets$id)

  # join summed crimes/tweets back to data.frame
  hex@data <- left_join(hex@data, pts.agg.tweets, by = c("ZoneID" = "id"))
  hex@data <- left_join(hex@data, pts.agg.crimes, by = c("ZoneID" = "id.1"))

  # correlate unique zone id sums
  uniques <- hex@data[!duplicated(hex@data$num.x), ]
  cor.tc <- cor(uniques$num.x, uniques$num.y, use="complete.obs")

  # rbind to master frame
  cors.zone.master <- rbind(cors.zone.master, cor.tc)
}
```

\begin{figure}[!htbp]
\centering
\includegraphics{IMG/cowplot_zon.jpeg}
\caption{Zone aggregations. Four iterations displayed A-D for 70 regions.}
\label{fig:zone}
\end{figure}

From \autoref{tab:zone}, we demonstrate the zoning problem: with 10 aggregation iterations for 70 regions we observe positive correlation between violent crime and tweets ranging from 0.42 (moderate assocation) to 0.96 (very strong association).

\begin{table}[!htbp]
\begin{center}
\caption{Correlation coefficients for 10 iterations of 70 regions.} \label{tab:zone}
\begin{tabular}{@{} *5l @{}}    \toprule
\emph{Iteration} & \emph{Correlation coeff.} &&&  \\\midrule
 1  & 0.89 \\ 
 2  & 0.66 \\ 
 3  & 0.61 \\ 
 4  & 0.89 \\ 
 5  & 0.42 \\ 
 6  & 0.96 \\ 
 7  & 0.75 \\ 
 8  & 0.92 \\ 
 9  & 0.82 \\ 
 10 & 0.84 \\\bottomrule
 \hline
\end{tabular}
\end{center}
\end{table}

\section{Multilevel Modelling}

Having demonstrated the MAUP, we implement a two-level multilevel model that accounts for spatial heterogeneity effects between the scale geographies. We use open data derived from the data.cdrc.ac.uk portal, in particular IMD 2015 and median house prices for 2015 data. LSOA and MSOA geographies are joined by location in `QGIS` as R lacks libraries for polygon in polygon joins. We begin by tiding the data, before log transforming median housing price per LSOA to relax the assumption of normality:

```{r data_prep, results="hide"}
setwd("/Users/samcomber/documents/spatial_analysis/shp/spatial_join")
# LSOA and MSOA geographies joined by location in QGIS as
# R lacks library for polygon.in.polygon joins
lsoa.join <- readOGR(dsn = ".", layer = "lsoa_join", stringsAsFactors = FALSE)
# take only dataframe
lsoa.join <- lsoa.join@data
# cast as integer
lsoa.join$md_2015 <- as.numeric(lsoa.join$md_2015)
# log housing prices
lsoa.join$md_2015 <- log(lsoa.join$md_2015)
# take only complete cases
lsoa.join <- lsoa.join[complete.cases(lsoa.join),]
```
 
\subsection{Null Model}

Generally, our goal of estimation is the partial-pooling estimates of the median log housing prices among all LSOAs in MSOA $j$. In our null model of no predictors, we approximate the multilevel estimate for MSOA $j$ as a weighted average of the mean of properties sold in the MSOA - an unpooled estimate $\hat{y}_j$ - and the mean across all MSOAs - the completely pooled estimate, $\hat{y}_\textnormal{all}$. As the weighted average uses information available about individual LSOAs, averages from MSOAs with a small number of constituent LSOAs carry less weighting, and so the multilevel estimate is pulled closer to Liverpool's overall average\footnote{In this way, the relative weights are determined by the sample size in the group and the variation within and between groups.} (Gelman, 2007). To allow this 'soft constraint' on $\alpha _j$, partial-pooling estimates are derived by assigning a probability distribution to the $\alpha_j$'s which pulls estimates of $\alpha _j$ some way towards their mean level $\mu _\alpha$ as below:

$$ \alpha_j \sim \mathcal{N}(\mu _\alpha,\,\sigma_{\alpha}^{2})\, \textnormal{for } j = 1, . . ., J, $$ 

where the mean $\mu _\alpha$ and standard deviation $\sigma _\alpha$ derived from the data. To begin decomposing how median log housing prices vary by LSOA and MSOA geographies, we specify the following null multilevel model:

$$ y_{ij} = \alpha_{0,j} + \epsilon_{ij} $$

$$ \alpha_{0j} = \alpha_0 + \mu_{0j} $$

where in the first level, $y_{ij}$ is the median log housing price for the $i^{th}$ LSOA in MSOA $j$, $\alpha_{0j}$ is the MSOA-level mean of $y_{ij}$ (varying intercept) for the $j^{th}$ MSOA, and $\epsilon_{ij}$ is the residual error term assumed i.i.d; where in the second level, $\alpha_0$ is the overall intercept, and finally $\mu_{0j}$ is the random error component for the deviation of the intercept of the $j^{th}$ group from the overall intercept.

```{r null model}
# ----------- NULL MODEL ------------

# (1|MSOA11CD) allows intercept (coefficient of 1 is the constant 
# term in the regression) to vary by MSOA
model.null <- lmer(md_2015 ~ 1 + (1|MSOA11CD), data = lsoa.join)

# get variance
vc <- VarCorr(model.null)
print(vc,comp=c("Variance"))

# calculate variance partition coefficient
print(paste0(round(0.18341 / (0.18341 + 0.12261), 3),"%"))
```

Having observed a Variance Partition Coefficient (VPC) of `0.599`, we approximate ~60% of the variation in median log housing prices as explained by inequalities between MSOAs, and ~40% within. This infers the presence of spatial heterogeneity between MSOAs, which justifies the use of a multilevel model to simultaneously capture outcomes at LSOA and MSOA geographies. 

Following this, we calculate rankings of posterior variance of each MSOA-level residual. From the values tabulated in \autoref{tab:rank}, we can observe that, for example, MSOA E02001347 had an estimated residual of `-0.3636` and was ranked 11 places from bottom. Hence, for E02001347, we estimate a mean log housing value of  `11.52383` `-` `-0.3636` = `11.8874` to 4 d.p.

```{r}
# store MSOA-level residuals
u0 <- ranef(model.null, condVar = TRUE)
u0se <- sqrt(attr(u0[[1]], "postVar")[1, , ])

# get msoa ids
msoa.id <- rownames(u0[[1]])
# column bind ids, residuals and standard errors
u0tab <- cbind(msoa.id, u0[[1]], u0se)
# rename columns
colnames(u0tab) <- c("msoa.id", "u0", "u0se")
# sort table by residuals
u0tab <- u0tab <- u0tab[order(u0tab$u0), ]

# create ranking
u0tab <- cbind(u0tab, c(1:dim(u0tab)[1]))
# rename column to rank
colnames(u0tab)[4] <- "u0rank"
# order by msoa id
u0tab <- u0tab[order(u0tab$msoa.id),]
```

\begin{table}[!htbp]
\begin{center}
\caption{Ranking of residuals for MSOAs.} \label{tab:rank}
\begin{tabular}{@{} *5l @{}}    \toprule
\emph{MSOA id} & \emph{u0} & \emph{u0se} & \emph{u0 rank} &  \\\midrule
 E02001347  & -0.3636 & 0.2144 & 11\\ 
 E02001348  & 0.0015 & 0.1621 & 27\\ 
 E02001349  & 0.0300 & 0.1356 & 28\\ 
 E02001350  & -0.4477 & 0.1621 & 5\\ 
 E02001351  & -0.1039 & 0.2711 & 21\\ 
 E02001352  & -0.0924 & 0.1621 & 22\\ 
 E02001353  & -0.0829 & 0.1471 & 25\\ 
 E02001355  & -0.2823 & 0.1471 & 13\\ 
 E02001357  & -0.3988 & 0.1621  & 8\\ 
 E02001358 & -0.5902 &  0.1828 & 3\\\bottomrule
 \hline
\end{tabular}
\end{center}
\end{table}

\subsection{Random-slopes model}

To rationalise a varying intercept and slope model, we assess the variability between median housing prices and the percentage of neighbourhood income deprivation.

\begin{figure}[!htbp]
\centering
\includegraphics{IMG/varian.jpeg}
\caption{Variability in income deprivation by median log housing price for 20 sampled MSOAs.}
\label{fig:varib}
\end{figure}

As \autoref{fig:varib} demonstrates variance between median log housing prices and percentage of neighbourhood income deprivation, we are able to motivate the use of a random slope multilevel model. As observed, given random intercept models assume parallel grouping lines, they perform poorly at fitting the data. For this reason, we allow coefficients of explanatory variables to vary for each MSOA, which can be understood as interactions between group-level indicators and an individual-level predictor (Gelman, 2007). This is achieved by adding a random term to the coefficient of $x_2$ which relaxes the constraint of $\beta _2$ as fixed across each MSOA, allowing the coefficient to vary randomly across groups. We specify our random-slope model as follows:

$$ y_{ij} = \alpha _{0j} + \beta _{1}x_{1,ij} + \beta _{2j}x_{2,ij} + \epsilon _{0,ij}$$

$$ \alpha _{0j} = \alpha _{0} + \mu_{0j} $$

$$ \beta _{2j} = \beta +1 + \mu_{1j}  $$

which can be re-expressed into a single level by substituting formulae for $\alpha _{0j}$ and $\beta _{2j}$ into $y_{ij}$ as:

$$ y_{ij} = \alpha _0 + \beta_1 x_{ij} + \mu_{0j} + \mu_{2j}x_{ij} + \epsilon_{ij} $$

Conceptually, this equation is similar to the null model, but the $\mu _{2j}x_{ij}$ term has been added to reflect the interaction between the $j^{th}$ MSOA and income deprivation predictor. The random effects $\mu_{0j}$ and $\mu_{2j}$ are assumed: 

$$\begin{bmatrix} 
\mu_{0j} \\\ \mu_{2j} \end{bmatrix} 
\sim \mathcal{N}(0,
\begin{bmatrix}
\sigma_{\mu0}^{2} & \\ 
\sigma_{\mu 0 \mu 2} & \sigma_{\mu2}^{2} 
\end{bmatrix})$$

meaning the slope of regression line for MSOA $j$ is $\beta_2 + \mu_{2j}$, which allows the income deprivation coefficient to vary by group. We define our random-slopes model in $R$ as follows:

```{r}
# --------- RANDOM SLOPES MODEL -----------
model.var.slope <- lmer(md_2015 ~ lr_crime + lr_income
                        + (1 + lr_income | MSOA11CD), data=lsoa.join, REML = FALSE)

model.var.slope.fit <- lmer(md_2015 ~ lr_crime + lr_income
                        + (1 | MSOA11CD), data=lsoa.join,  REML = FALSE)

# compute anova for likelihood ratio
anova(model.var.slope, model.var.slope.fit)
```

Firstly, we use a likelihood ratio test derived from an ANOVA table to estimate whether the income deprivation effect varies across MSOAs. As the deduction of the log-likelihood values computes a likelihood ratio value of `0.4958` on 2 degrees of freedom, we can infer the income deprivation effect varies across MSOAs, justifying the use of a random-slopes model. 

```{r}
summary(model.var.slope)
```

To begin interpreting the intercept and slope residuals, we graphically project the plot of income deprivation slopes by intercept ($\hat{\mu}_{2j}$ by $\hat{\mu}_{0j}$) in \autoref{fig:slope}. In all, the plot somewhat conforms with conventional wisdom:  

<!-- 

bottom-right =
bottom-left =

top-left =
top-right =

-->

MSOAs with higher-than-average median log housing prices also have below-average slopes for income deprivation as shown in the bottom-right quadrant. Yet, surprisingly, 

```{r, fig.cap = "\\label{fig:slope}Income deprivation slopes versus intercept."}
# data object containing random slope and intercepts
inc.random <- ranef(model.var.slope, condVar = TRUE)

plot(inc.random[[1]], xlab = "Intercept (u0j)", ylab = "Slope of lr_income (u2j)")
abline(h = 0, col = "red")
abline(v = 0, col = "red")
```

In all, the mean fixed effect slope for `lr_income` infers that every one-unit increase in the percentage of income deprivation decreases median housing values by 25.1\%\footnote{Median housing values are logged so the coefficients are interpreted as percentage increases.}. As we do not allow `lr_income` to vary by group, we can interpret the coefficient as stating for every one-unit increase in the percentage of crime deprivation, median housing values decrease by 0.42\%. In this way, one might conclude income deprivation has a greater effect size on median housing values than crime deprivation. To produce the equation for the fitted regression line of MSOA $j$, one can solve: $\hat{y} _{ij} =  (12.22 + \hat{\mu}_{0j}) + (-0.04)\textnormal{lr\_crime} + (-2.50 + \hat{\mu_{2j}})\textnormal{lr\_income}$ where `12.22`, `-0.04` and `-2.50` are derived from the fixed effects estimates of the intercept `lr_crime` and `lr_income`, respectively, and the values of $\hat{\mu}_{0j}$ and $\hat\mu_{2j}$ can be derived from the pairwise residual plot in \autoref{fig:slope}. 

Finally, we compute a caterpillar plot in \autoref{fig:caterpillar} to display the MSOA effects in rank order. From this plot, we observe how the random intercepts does not seem to vary significantly with eachother as indicated by the overlapping 95\% confidence intervals with the exception of one MSOA. Moreover, the effects of income deprivation on median log housing prices does not seem to vary significantly across MSOAs: positive and negative effects are observed, yet they are not significantly different between MSOAs. This infers the absence of between-group significant variance in the effect of income deprivation on median log housing values.

```{r catepillar, fig.cap = "\\label{fig:caterpillar}Mean of MSOA random effect with 95% confidence interval."}
# create caterpillar graph 
msoa.rand <- REsim(model.var.slope)
plotREsim(msoa.rand)
```

\newpage
\subsection{Space and conclusions}

With our interest in space, we conclude this paper by briefly displaying the spatial distribution of random slopes in \autoref{fig:spplot} using `spplot` for the presentation layer. From this, we observe the spatiality of slope coefficients for income deprivation across Liverpool. In terms of spatial patterning, one can observe clusters of the highest coefficients amongst MSOAs to the West of Liverpool. This infers that within these MSOAs, increasing income deprivation somewhat counterintuitively increased median housing values\footnote{This might be due to confounding factors not captured by the multilevel model.}. By contrast, MSOAs around the center of the Liverpool metropolitan area conform with what one would assume a priori: increasing income deprivation causes median housing values to fall.

```{r spplot, fig.cap = "\\label{fig:spplot}Spatial patterning of varying slopes at MSOA scale.", echo=FALSE, results="hide", warning=FALSE}
setwd("/Users/samcomber/Documents/spatial_analysis/shp/spatial_join")
msoa <- readOGR(dsn = ".", layer = "msoa", stringsAsFactors = FALSE)
msoa_code <- msoa.rand[msoa.rand$term == "(Intercept)", "groupID"]
msoa_intercept <- msoa.rand[msoa.rand$term == "(Intercept)", "mean"]
msoa_slope <- msoa.rand[msoa.rand$term == "lr_income", "mean"]
msoa_link <- data.frame(msoa_code, msoa_intercept, msoa_slope)
msoa_link$msoa_code <- as.character(msoa_link$msoa_code)
msoa@data <- merge(msoa@data, msoa_link, by.x = "MSOA11CD", by.y = "msoa_code", all.x = TRUE)

temp <- msoa@data$msoa_slope
brks.temp <- classIntervals(temp, n = 5, style = "quantile")
brks.temp <- round(brks.temp$brks, digits = 2)
brks.temp[1] <- brks.temp[1] - 0.01
brks.temp[5 + 1] <- brks.temp[5 + 1] + 0.01

# get bounding box
xx <- msoa@bbox

# build scale bar
scalebar <- list("SpatialPolygonsRescale", layout.scale.bar(),
            offset = c(min(xx[1,] + 1000),xx[2,1] + 1000), scale = 5000,    fill=c("transparent","black"))

text1 <- list("sp.text", c(min(xx[1,]) + 1000,xx[2,1] + 1600), "0")
text2 <- list("sp.text", c(min(xx[1,]) + 6000,xx[2,1] + 1600), "5 km")
northarrow <- list("SpatialPolygonsRescale", layout.north.arrow(),
                     offset = c(min(xx[1,]) + 2000,xx[2,1] + 2000), scale = 1500)


spplot(msoa, "msoa_slope", sp.layout = list(scalebar, text1, text2, northarrow), at = brks.temp, col.regions = brewer.pal(5, "PuBu"), col="transparent", pretty=TRUE, colorkey=list(labels=list(at=brks.temp)))

```

\newpage
\section{Bibliography}

* Clark, W. and Avery, K. (1976) The effects of data aggregation in statistical analysis. <i>Geographical Analysis</i>, Vol.8 p-428-438.
* Fotheringham A. and Wong, D. (1991) The Modifiable Areal Unit Problem in Multivariate Statistical Analysis. <i>Environment and planning A</i>. Vol. 23 (7), p.1025-1044.
* Gelman, A. (2007) <i>Data Analysis using Regression and Multilevel/Hierarchical Models</i>. Cambridge University Press: Camrbidge.
* Openshaw S, (1984) <i>The Modifiable Areal Unit Problem - Concepts and Techniques in Modern Geography</i>. Geo Books: Norwich. 

\newpage
\section{Appendix}

\textbf{1.1 - Master dataframe construction code.}

```{r hidden_render, eval=FALSE}
# build master data.frame to append data.frame for individual months to
master <- data.frame(id=numeric(0), category=character(0), lat=character(0),
                     lon=character(0), month=character(0), outcome_status=character(0))

d1 <- data.frame(category=document[[1]]$category,lat=document[[1]]$location$latitude,
                 lon=document[[1]]$location$longitude, id=document[[1]]$id, 
                 name=document[[1]]$location$street$name, month=document[[1]]$month,
                 outcome_status=document[[1]]$outcome_status$category)

d2 <- data.frame(category=document[[2]]$category,lat=document[[2]]$location$latitude,
                 lon=document[[2]]$location$longitude, id=document[[2]]$id,
                 name=document[[2]]$location$street$name, month=document[[2]]$month,
                 outcome_status=document[[2]]$outcome_status$category)

d3 <- data.frame(category=document[[3]]$category,lat=document[[3]]$location$latitude,
                 lon=document[[3]]$location$longitude, id=document[[3]]$id,
                 name=document[[3]]$location$street$name, month=document[[3]]$month,
                 outcome_status=document[[3]]$outcome_status$category)

d4 <- data.frame(category=document[[4]]$category,lat=document[[4]]$location$latitude,
                 lon=document[[4]]$location$longitude, id=document[[4]]$id,
                 name=document[[4]]$location$street$name, month=document[[4]]$month,
                 outcome_status=document[[4]]$outcome_status$category)

d5 <- data.frame(category=document[[5]]$category,lat=document[[5]]$location$latitude,
                 lon=document[[5]]$location$longitude, id=document[[5]]$id,
                 name=document[[5]]$location$street$name, month=document[[5]]$month,
                 outcome_status=document[[5]]$outcome_status$category)

d6 <- data.frame(category=document[[6]]$category,lat=document[[6]]$location$latitude,
                 lon=document[[6]]$location$longitude, id=document[[6]]$id,
                 name=document[[6]]$location$street$name, month=document[[6]]$month,
                 outcome_status=document[[6]]$outcome_status$category)

d7 <- data.frame(category=document[[7]]$category,lat=document[[7]]$location$latitude,
                 lon=document[[7]]$location$longitude, id=document[[7]]$id,
                 name=document[[7]]$location$street$name, month=document[[7]]$month,
                 outcome_status=document[[7]]$outcome_status$category)

d8 <- data.frame(category=document[[8]]$category,lat=document[[8]]$location$latitude,
                 lon=document[[8]]$location$longitude, id=document[[8]]$id,
                 name=document[[8]]$location$street$name, month=document[[8]]$month,
                 outcome_status=document[[8]]$outcome_status$category)

d9 <- data.frame(category=document[[9]]$category,lat=document[[9]]$location$latitude,
                 lon=document[[9]]$location$longitude, id=document[[9]]$id,
                 name=document[[9]]$location$street$name, month=document[[9]]$month,
                 outcome_status=document[[9]]$outcome_status$category)

d10 <- data.frame(category=document[[10]]$category,lat=document[[10]]$location$latitude,
                  lon=document[[10]]$location$longitude, id=document[[10]]$id,
                  name=document[[10]]$location$street$name, month=document[[10]]$month,
                  outcome_status=document[[10]]$outcome_status$category)

d11 <- data.frame(category=document[[11]]$category,lat=document[[11]]$location$latitude,
                  lon=document[[11]]$location$longitude, id=document[[11]]$id,
                  name=document[[11]]$location$street$name, month=document[[11]]$month,
                  outcome_status=document[[11]]$outcome_status$category)

d12 <- data.frame(category=document[[12]]$category,lat=document[[12]]$location$latitude,
                  lon=document[[12]]$location$longitude, id=document[[12]]$id,
                  name=document[[12]]$location$street$name, month=document[[12]]$month,
                  outcome_status=document[[12]]$outcome_status$category)

# rbind each month to master data.frame
document <- rbind(master, d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12)
# document <- rbind(master, d1, d2)

# take a sample from population
document <- document[sample(nrow(document), 45000),]
# convert factors to chr class
document <- rapply(document, as.character, classes="factor", how="replace")
```