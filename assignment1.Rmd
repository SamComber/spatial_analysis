---
title: "Assignment Two: The MAUP and Multilevel Modelling"
output:
  pdf_document:
    toc: true
    number_sections: true
bibliography: references.bib
header-includes:
   - \setlength\parindent{24pt}
---

\section{Demonstrating the MAUP}

\subsection{Background}

<!-- introduce problem -->

Areal units in zoning systems amalgamate into objects that constitute the basic units for the observation and analysis of spatial phenomena [@Openshaw]. Yet, no gold standard for guiding the spatial aggregation process exists, with the validity of zonal objects subject to the arbitrary and modifiable decision-making of quantitative geographers. Problematically, the analysis of socioeconomic data involving areal units is encumbered by the modifiable areal unit problem (MAUP): "the sensitivity of analytical results to the definition of units for which data are collected." According to the literature, the MAUP constrains the reliability of analyses for aggregated spatial data, as findings have shown varying results with the scale of aggregation and configuration of zoning systems [@ClarkAvery].

<!-- zoning problem + examples -->
<!-- aggregation problem + examples -->

In practice, the MAUP is condensed into two issues of scale and zoning sensitivity which this paper will attempt to demonstrate in Section \ref{maup}. The first issue, described as the *scale problem*, is the variation in findings when data for zonal units are progressively aggregated. This has been demonstrated empirically by @ClarkAvery who found that whilst correlation coefficients did not increase monotonically with aggregation\footnote{At higher levels of aggregation, there is smaller adjacency of zonal units meaning groupings are more heterogenous leading to lower correlation coefficients.}, a general increase in data aggregation corresponds to an increase in correlation coefficients. 

The second issue, the *zoning problem*, pertains to the variation in findings when alternative combinations of zonal units are analysed with the scale or number of units held constant [@Openshaw]. Zoning sensitivity in multivariate analysis has been demonstrated empirically in @FotheringhamWong who simulated the aggregation of 871 block groups into 218 zones in 150 different iterations. They highlight the severity of the zoning problem by demonstrating the possibility of concluding with one iteration of zones no association between the percentage of blue-collar workers and mean family income, with another iteration finding a unit increase in blue-collar worker percentages as reducing mean family income by \$20,000. In all, ignoring scale and zoning sensitivity in model calibration can lead to inferential conclusions that a researcher's areal data is applicable to the constituents who form the zones under study - the ecological fallacy problem [@Openshaw].

\subsection{MAUP analysis}
\label{maup}

\subsubsection{Data}

To demonstrate the scaling and zoning sensitivities of the MAUP, we calculate the bivariate strength of association between two open data variables. For our first variable, *crime_count*, we submit a HTTP request using the POST verb to send a custom polygon for retrieving all street-level crimes occuring in 2012. 

```{r dependencies, include=FALSE}
library(downloader)
library(rgdal)
library(httr)
library(ggplot2)
library(ggmap)
library(sp)
library(maptools)
library(classInt)
library(RColorBrewer)
library(lme4)
library(merTools)
library(dplyr)
library(spatialEco)
```

```{r crime_data, eval=FALSE}
# download geojson
u <- "http://statistics.data.gov.uk/boundaries/E08000012.json"
# store in temporary directory 
downloader::download(url = u, destfile = "/tmp/lpool.geojson")
lpool <- readOGR(dsn = "/tmp/lpool.geojson", layer = "OGRGeoJSON")
# access coords slot
lpool <- lpool@polygons[[1]]@Polygons[[1]]@coords
# build lat/lon + date string to send with postrequest
curl.string <- paste0('poly=',paste0(sprintf('%s,%s',lpool[,2], lpool[,1])
                                     , collapse = ':'))

# build dates list for loop 
dates = c("2012-01", "2012-02", "2012-03", "2012-04", "2012-05", "2012-06",
          "2012-07", "2012-08", "2012-09", "2012-10", "2012-11", "2012-12")

document <- lapply(dates, function(month) {
  # format acceptable packet for http request
  curl.string <- list(poly=c(curl.string), date=c(month))
  # post custom polygon to police api 
  r <- httr::POST("https://data.police.uk/api/crimes-street/all-crime", 
                  body = curl.string, encode="multipart", verbose())
  json <- content(r, "text", encoding = "ISO-8859-1")
  # return as data.frame
  jsonlite::fromJSON(txt=json)
})
```

```{r hidden_render, eval=FALSE, include=FALSE}
# build master data.frame to append data.frame for individual months to
master <- data.frame(id=numeric(0), category=character(0), lat=character(0),
                     lon=character(0), month=character(0), outcome_status=character(0))

d1 <- data.frame(category=document[[1]]$category,lat=document[[1]]$location$latitude,
                 lon=document[[1]]$location$longitude, id=document[[1]]$id, 
                 name=document[[1]]$location$street$name, month=document[[1]]$month,
                 outcome_status=document[[1]]$outcome_status$category)

d2 <- data.frame(category=document[[2]]$category,lat=document[[2]]$location$latitude,
                 lon=document[[2]]$location$longitude, id=document[[2]]$id,
                 name=document[[2]]$location$street$name, month=document[[2]]$month,
                 outcome_status=document[[2]]$outcome_status$category)

d3 <- data.frame(category=document[[3]]$category,lat=document[[3]]$location$latitude,
                 lon=document[[3]]$location$longitude, id=document[[3]]$id,
                 name=document[[3]]$location$street$name, month=document[[3]]$month,
                 outcome_status=document[[3]]$outcome_status$category)

d4 <- data.frame(category=document[[4]]$category,lat=document[[4]]$location$latitude,
                 lon=document[[4]]$location$longitude, id=document[[4]]$id,
                 name=document[[4]]$location$street$name, month=document[[4]]$month,
                 outcome_status=document[[4]]$outcome_status$category)

d5 <- data.frame(category=document[[5]]$category,lat=document[[5]]$location$latitude,
                 lon=document[[5]]$location$longitude, id=document[[5]]$id,
                 name=document[[5]]$location$street$name, month=document[[5]]$month,
                 outcome_status=document[[5]]$outcome_status$category)

d6 <- data.frame(category=document[[6]]$category,lat=document[[6]]$location$latitude,
                 lon=document[[6]]$location$longitude, id=document[[6]]$id,
                 name=document[[6]]$location$street$name, month=document[[6]]$month,
                 outcome_status=document[[6]]$outcome_status$category)

d7 <- data.frame(category=document[[7]]$category,lat=document[[7]]$location$latitude,
                 lon=document[[7]]$location$longitude, id=document[[7]]$id,
                 name=document[[7]]$location$street$name, month=document[[7]]$month,
                 outcome_status=document[[7]]$outcome_status$category)

d8 <- data.frame(category=document[[8]]$category,lat=document[[8]]$location$latitude,
                 lon=document[[8]]$location$longitude, id=document[[8]]$id,
                 name=document[[8]]$location$street$name, month=document[[8]]$month,
                 outcome_status=document[[8]]$outcome_status$category)

d9 <- data.frame(category=document[[9]]$category,lat=document[[9]]$location$latitude,
                 lon=document[[9]]$location$longitude, id=document[[9]]$id,
                 name=document[[9]]$location$street$name, month=document[[9]]$month,
                 outcome_status=document[[9]]$outcome_status$category)

d10 <- data.frame(category=document[[10]]$category,lat=document[[10]]$location$latitude,
                  lon=document[[10]]$location$longitude, id=document[[10]]$id,
                  name=document[[10]]$location$street$name, month=document[[10]]$month,
                  outcome_status=document[[10]]$outcome_status$category)

d11 <- data.frame(category=document[[11]]$category,lat=document[[11]]$location$latitude,
                  lon=document[[11]]$location$longitude, id=document[[11]]$id,
                  name=document[[11]]$location$street$name, month=document[[11]]$month,
                  outcome_status=document[[11]]$outcome_status$category)

d12 <- data.frame(category=document[[12]]$category,lat=document[[12]]$location$latitude,
                  lon=document[[12]]$location$longitude, id=document[[12]]$id,
                  name=document[[12]]$location$street$name, month=document[[12]]$month,
                  outcome_status=document[[12]]$outcome_status$category)

# rbind each month to master data.frame
document <- rbind(master, d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12)
# document <- rbind(master, d1, d2)

# take a sample from population
document <- document[sample(nrow(document), 45000),]
# convert factors to chr class
document <- rapply(document, as.character, classes="factor", how="replace")
```

```{r, eval=FALSE}
# cast lat/lon columns to numeric data type
document$lat <- as.numeric(document$lat)
document$lon <- as.numeric(document$lon)

# convert data.frame to shapefile in rgdal
coordinates(document) <- ~lon+lat
proj4string(document) <- CRS("+init=epsg:4326") # WGS 84

# write once
writeOGR(document, "/Users/samcomber/Documents/spatial_analysis/shp/crimes",
         "crimes", driver = "ESRI Shapefile")

crimes <- readOGR(dsn = "/Users/samcomber/Documents/spatial_analysis/shp/crimes",
                  layer = "crimes")
```

Regarding our second variable, *tweet_count*, we aggregate geo-referenced tweets containing timestamps relating to Twitter postings within the municipality of Liverpool for 2012\footnote{This dataset was data mined by Guy Lansley from UCL, and processed by Dani Arribas-Bel.}.

```{r twitter_data, eval=FALSE}
tweets <- readOGR(dsn = "/Users/samcomber/Documents/spatial_analysis/shp/tweets",
                  layer = "tweets_liverpool")
```

<!-- explain IMAGE a bit more -->

To demonstrate the MAUP, we aggregate a count of each crime and tweet into separate variables for each region in the shapefile, before computing correlations between the two vectors of values in the dataframe. Specifically, we investigate the scaling problem using a hex binned lattice which **. Finally, we demonstrate the zoning problem by constraining cardinality - i.e. the number of regions per shapefile - to 100. We generate 10 iterations of 100 regions using Openshaw's depth-first algorithm\footnote{A visualisation of the contiguity graph is showed below.} in IMAGE studio [@Kostas]. 

\subsubsection{MAUP findings}

<!-- SCALE PROBLEM -->

```{r, eval=FALSE}


```

<!-- ZONING PROBLEM -->

```{r, eval=FALSE}
hex <- readOGR(dsn = "/Users/samcomber/Documents/spatial_analysis", layer = "hex_test")

# points in polygon spatial join
pts.poly <- point.in.poly(crimes, hex)

# aggregate crimes data
pts.agg <- pts.poly@data %>% group_by(z010_i01_Z) %>% summarise(num = n()) %>% arrange(z010_i01_Z)

# join summed crimes back to data.frame
hex@data <- left_join(hex@data, pts.agg, by = c("z010_i01_Z" = "z010_i01_Z"))

# 
spplot(hex, "num")
```

\section{Multilevel Modelling}

In this section, we implement a two-level multilevel model that accounts for spatial heterogeneity effects between the levels.

\subsection{Model specification}

<!-- variables decision -->

```{r, eval=FALSE}
# msoa and lsoa geographies were joined by location in QGIS
# as R lacks any robust library for polygon.in.polygon joins
lsoa.join <- readOGR(dsn = 
                     "/Users/samcomber/Documents/spatial_analysis/shp/spatial_join",
                     layer = "lsoa_join", stringsAsFactors = FALSE)

# save location to tmp folder
temp <- "/tmp/lpool_pp.csv"

# download from source
download.file(paste0("http://landregistry.data.gov.uk/app/ppd/ppd_data.csv
                     ?et%5B%5D=lrcommon%3Afreehold&et%5B%5D=lrcommon%3Aleasehold
                     &header=true&limit=all&max_date=31+December+2016
                     &min_date=1+January+2016&nb%5B%5D=true&nb%5B%5D=false
                     &ptype%5B%5D=lrcommon%3Adetached&ptype%5B%5D=lrcommon%3Asemi-detached
                     &ptype%5B%5D=lrcommon%3Aterraced&ptype%5B%5D=lrcommon%
                     3Aflat-maisonette&ptype%5B%5D=lrcommon%3AotherPropertyType
                     &tc%5B%5D=ppd%3AstandardPricePaidTransaction
                     &tc%5B%5D=ppd%3AadditionalPricePaidTransaction&town=",
                     "Liverpool"), temp)

# load LR data
lr <- read.csv("/tmp/lpool_pp.csv",
               stringsAsFactors = FALSE)

# select only relevant columns
lr <- dplyr::select(lr, price_paid, property_type, new_build, saon, paon, street,
                    locality, town, postcode)

# add concatenated column for geocoding
lr.geocode <- mutate(lr, address = paste(saon, paon, street, locality, town, postcode))

# drop previous columns
lr.geocode <- dplyr::select(lr.geocode, -saon, -paon, -street, 
                            -locality, -town, -postcode)

# take sample to prevent API query limit from Google (2,500)
lr.geocode.sample <- lr.geocode[sample(nrow(lr.geocode), 2450),]

# loop over address, get lat/lon
geo.done <- data.frame()
for (i in 1:nrow(lr.geocode.sample)) {
  g <- geocode(lr.geocode.sample$address[i])
  g <- mutate(g, price=lr.geocode.sample$price_paid[i])
  geo.done <- rbind(geo.done, g)
}

# export to csv for backup
write.csv(geo.done, file = "/Users/samcomber/Documents/spatial_analysis/data/
          geocoded_lr.csv", row.names=FALSE)

# convert to spatial points df

```

** avg housing price over areal unit **

<!-- model specification(s) -->

$$ Price_{i,j} = \beta $$

\subsection{Analysis}
\subsection{Interpretation}

\section{Bibliography}
\section{Appendix}