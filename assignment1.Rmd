---
title: "Assignment One: The MAUP and Multilevel Modelling"
output:
  pdf_document:
    toc: true
    number_sections: true
bibliography: references.bib
header-includes:
   - \setlength\parindent{24pt}
   - \usepackage[table]{xcolor}
   - \usepackage{booktabs}
   - \usepackage{subfig}
---

\section{Demonstrating the MAUP}

\subsection{Background}

<!-- introduce problem -->

Areal units in zoning systems amalgamate into objects that constitute the basic units for the observation and analysis of spatial phenomena [@Openshaw]. Yet, no gold standard for guiding the spatial aggregation process exists, with the validity of zonal objects subject to the arbitrary and modifiable decision-making of quantitative geographers. Problematically, the analysis of socioeconomic data involving areal units is encumbered by the modifiable areal unit problem (MAUP): "the sensitivity of analytical results to the definition of units for which data are collected." According to the literature, the MAUP constrains the reliability of analyses for aggregated spatial data, as findings have shown varying results with the scale of aggregation and configuration of zoning systems [@ClarkAvery].

<!-- zoning problem + examples -->
<!-- aggregation problem + examples -->

In practice, the MAUP is condensed into two issues of scale and zoning sensitivity which this paper will attempt to demonstrate in Section \ref{maup}. The first issue, described as the *scale problem*, is the variation in findings when data for zonal units are progressively aggregated. This has been demonstrated empirically by @ClarkAvery who found that whilst correlation coefficients did not increase monotonically with aggregation\footnote{At higher levels of aggregation, there is smaller adjacency of zonal units meaning groupings are more heterogenous leading to lower correlation coefficients.}, a general increase in data aggregation corresponds to an increase in correlation coefficients. 

The second issue, the *zoning problem*, pertains to the variation in findings when alternative combinations of zonal units are analysed with the scale or number of units held constant [@Openshaw]. Zoning sensitivity in multivariate analysis has been demonstrated empirically in @FotheringhamWong who simulated the aggregation of 871 block groups into 218 zones in 150 different iterations. They highlight the severity of the zoning problem by demonstrating the possibility of concluding with one iteration of zones no association between the percentage of blue-collar workers and mean family income, with another iteration finding a unit increase in blue-collar worker percentages as reducing mean family income by \$20,000. In all, ignoring scale and zoning sensitivity in model calibration can lead to inferential conclusions that a researcher's areal data is applicable to the constituents who form the zones under study - the ecological fallacy problem [@Openshaw].

\subsection{Demonstrating the MAUP}
\label{maup}

\subsubsection{Data}

To demonstrate the scaling and zoning sensitivities of the MAUP, we calculate the bivariate strength of association between two open data variables. For our first variable, *crime_count*, we submit a HTTP request using the POST verb to send a custom polygon for retrieving all street-level crimes occuring in 2012. 

```{r dependencies, include=FALSE}
library(downloader)
library(rgdal)
library(httr)
library(ggplot2)
library(ggmap)
library(sp)
library(maptools)
library(classInt)
library(RColorBrewer)
library(lme4)
library(merTools)
library(dplyr)
library(spatialEco)
library(cowplot)
```

```{r crime_data, eval=FALSE}
# download geojson
u <- "http://statistics.data.gov.uk/boundaries/E08000012.json"
# store in temporary directory 
downloader::download(url = u, destfile = "/tmp/lpool.geojson")
lpool <- readOGR(dsn = "/tmp/lpool.geojson", layer = "OGRGeoJSON")
# access coords slot
lpool <- lpool@polygons[[1]]@Polygons[[1]]@coords
# build lat/lon + date string to send with postrequest
curl.string <- paste0('poly=',paste0(sprintf('%s,%s',lpool[,2], lpool[,1])
                                     , collapse = ':'))

# build dates list for loop 
dates = c("2012-01", "2012-02", "2012-03", "2012-04", "2012-05", "2012-06",
          "2012-07", "2012-08", "2012-09", "2012-10", "2012-11", "2012-12")

document <- lapply(dates, function(month) {
  # format acceptable packet for http request
  curl.string <- list(poly=c(curl.string), date=c(month))
  # post custom polygon to police api 
  r <- httr::POST("https://data.police.uk/api/crimes-street/all-crime", 
                  body = curl.string, encode="multipart", verbose())
  json <- content(r, "text", encoding = "ISO-8859-1")
  # return as data.frame
  jsonlite::fromJSON(txt=json)
})
```

```{r hidden_render, eval=FALSE, include=FALSE}
# build master data.frame to append data.frame for individual months to
master <- data.frame(id=numeric(0), category=character(0), lat=character(0),
                     lon=character(0), month=character(0), outcome_status=character(0))

d1 <- data.frame(category=document[[1]]$category,lat=document[[1]]$location$latitude,
                 lon=document[[1]]$location$longitude, id=document[[1]]$id, 
                 name=document[[1]]$location$street$name, month=document[[1]]$month,
                 outcome_status=document[[1]]$outcome_status$category)

d2 <- data.frame(category=document[[2]]$category,lat=document[[2]]$location$latitude,
                 lon=document[[2]]$location$longitude, id=document[[2]]$id,
                 name=document[[2]]$location$street$name, month=document[[2]]$month,
                 outcome_status=document[[2]]$outcome_status$category)

d3 <- data.frame(category=document[[3]]$category,lat=document[[3]]$location$latitude,
                 lon=document[[3]]$location$longitude, id=document[[3]]$id,
                 name=document[[3]]$location$street$name, month=document[[3]]$month,
                 outcome_status=document[[3]]$outcome_status$category)

d4 <- data.frame(category=document[[4]]$category,lat=document[[4]]$location$latitude,
                 lon=document[[4]]$location$longitude, id=document[[4]]$id,
                 name=document[[4]]$location$street$name, month=document[[4]]$month,
                 outcome_status=document[[4]]$outcome_status$category)

d5 <- data.frame(category=document[[5]]$category,lat=document[[5]]$location$latitude,
                 lon=document[[5]]$location$longitude, id=document[[5]]$id,
                 name=document[[5]]$location$street$name, month=document[[5]]$month,
                 outcome_status=document[[5]]$outcome_status$category)

d6 <- data.frame(category=document[[6]]$category,lat=document[[6]]$location$latitude,
                 lon=document[[6]]$location$longitude, id=document[[6]]$id,
                 name=document[[6]]$location$street$name, month=document[[6]]$month,
                 outcome_status=document[[6]]$outcome_status$category)

d7 <- data.frame(category=document[[7]]$category,lat=document[[7]]$location$latitude,
                 lon=document[[7]]$location$longitude, id=document[[7]]$id,
                 name=document[[7]]$location$street$name, month=document[[7]]$month,
                 outcome_status=document[[7]]$outcome_status$category)

d8 <- data.frame(category=document[[8]]$category,lat=document[[8]]$location$latitude,
                 lon=document[[8]]$location$longitude, id=document[[8]]$id,
                 name=document[[8]]$location$street$name, month=document[[8]]$month,
                 outcome_status=document[[8]]$outcome_status$category)

d9 <- data.frame(category=document[[9]]$category,lat=document[[9]]$location$latitude,
                 lon=document[[9]]$location$longitude, id=document[[9]]$id,
                 name=document[[9]]$location$street$name, month=document[[9]]$month,
                 outcome_status=document[[9]]$outcome_status$category)

d10 <- data.frame(category=document[[10]]$category,lat=document[[10]]$location$latitude,
                  lon=document[[10]]$location$longitude, id=document[[10]]$id,
                  name=document[[10]]$location$street$name, month=document[[10]]$month,
                  outcome_status=document[[10]]$outcome_status$category)

d11 <- data.frame(category=document[[11]]$category,lat=document[[11]]$location$latitude,
                  lon=document[[11]]$location$longitude, id=document[[11]]$id,
                  name=document[[11]]$location$street$name, month=document[[11]]$month,
                  outcome_status=document[[11]]$outcome_status$category)

d12 <- data.frame(category=document[[12]]$category,lat=document[[12]]$location$latitude,
                  lon=document[[12]]$location$longitude, id=document[[12]]$id,
                  name=document[[12]]$location$street$name, month=document[[12]]$month,
                  outcome_status=document[[12]]$outcome_status$category)

# rbind each month to master data.frame
document <- rbind(master, d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12)
# document <- rbind(master, d1, d2)

# take a sample from population
document <- document[sample(nrow(document), 45000),]
# convert factors to chr class
document <- rapply(document, as.character, classes="factor", how="replace")
```

```{r, eval=FALSE}
# cast lat/lon columns to numeric data type
document$lat <- as.numeric(document$lat)
document$lon <- as.numeric(document$lon)

# convert data.frame to shapefile in rgdal
coordinates(document) <- ~lon+lat
proj4string(document) <- CRS("+init=epsg:4326") # WGS 84

# subsetting operations
document <- document[document$category == "violent-crime",]

# write once
writeOGR(document, "/Users/samcomber/Documents/spatial_analysis/shp/crimes",
         "crimes", driver = "ESRI Shapefile")

crimes <- readOGR(dsn = "/Users/samcomber/Documents/spatial_analysis/shp/crimes",
                  layer = "crimes")
crimes <- crimes[sample(nrow(crimes), 500), ]
```

Regarding our second variable, *tweet_count*, we aggregate geo-referenced tweets containing timestamps relating to Twitter postings within the municipality of Liverpool for 2012\footnote{This dataset was data mined by Guy Lansley from UCL, and processed by Dani Arribas-Bel.}.

```{r twitter_data, eval=FALSE}
# read twitter shapefile
tweets <- readOGR(dsn = "/Users/samcomber/Documents/spatial_analysis/shp/tweets",
                  layer = "tweets_mersey")

# sample 500 tweets
tweets <- tweets[sample(nrow(tweets), 500), ]
```
```{r, eval=FALSE,include=FALSE}
# alternate x and y for correct aggregation (oops)
lats <- tweets@coords[, 1]
lon <- tweets@coords[, 2]
tweets@coords[,1] <- lon
tweets@coords[,2] <- lats

tweets <- tweets[sample(nrow(tweets), 500), ]
```

To demonstrate the MAUP, we aggregate a count of each crime and tweet into separate variables for each region in the shapefile, before computing correlations between the two vectors of values in the dataframe. We investigate the scaling problem using an iterative process that increments the number of regions in a hex-binned lattice from 10 to 100 (stepping each iteration by 10 regions). To demonstrate the zoning problem we constrain cardinality - i.e. the number of regions - to 30, and calculate correlation coefficients using 10 different permutations of 30 regions. To build random regions, we use the multiple aggregatons feature in IMAGE studio [@KDaras] which uses a depth-first search algorithm for a given contiguity matrix to create aggregated spatial regions. We then dissolve regions with the `unionSpatialPolygons` using the generated `ZoneID` for aggregations.

\subsubsection{Scale problem}

```{r, eval=FALSE}
# directory loop
folders <- dir("/Users/samcomber/Documents/spatial_analysis/shp/img_stud/run1/")

# we have folders + file names and are in run1 directory
setwd("/Users/samcomber/Documents/spatial_analysis/shp/img_stud/run1/")

# create master dataframe to hold correlation coefficents
cors.scale.master <- data.frame()

# loop over aggregation folder - i.e. 10 regions, 20, 30
for (folder in folders) {
  setwd(paste0("/Users/samcomber/Documents/spatial_analysis/shp/img_stud/run1","/"
               ,folder))
  
  # reread hex shapefile everytime we move folder
  hex <- readOGR(dsn = "/Users/samcomber/Documents/spatial_analysis/shp/hex", 
                 layer = "hex_1_clip")
  
  out <- data.frame()
  files <- list.files(pattern = "*.zon")
  
  # pick first iteration from each scale of aggregation
  img <- read.csv(files[1])

  # join regions to hex shapefile on area id
  hex@data$id <- as.numeric(hex@data$id)
  hex@data <- left_join(hex@data, img, by = c("id"= "AreaID"))
  hex@data <- hex@data[order(hex@data$id),]
  
  # dissolve by area id
  tes <- unionSpatialPolygons(hex, hex@data$ZoneID)

  # convert spatial polygons to spatialpolygonsdataframe
  oldw <- getOption("warn")
  
  # silence irrelevant warnings
  options(warn = -1)
  df <- data.frame(id = getSpPPolygonsIDSlots(tes))
  row.names(df) <- getSpPPolygonsIDSlots(tes)
  options(warn = oldw)
  
  spdf <- SpatialPolygonsDataFrame(tes, data = df)
  
  # points in polygon spatial join
  pts.crimes <- point.in.poly(crimes, spdf)
  pts.tweets <- point.in.poly(tweets, spdf)
  
  # aggregate crimes/tweets data
  pts.agg.crimes <- pts.crimes@data %>% group_by(id.1) %>% summarise(num = n()) 
  %>% arrange(id.1)
  pts.agg.tweets <- pts.tweets@data %>% group_by(id) %>% summarise(num = n()) 
  %>% arrange(id)
  
  # cast ids as factors for left_join
  pts.agg.crimes$id.1 <- as.integer(pts.agg.crimes$id.1)
  pts.agg.tweets$id <- as.integer(pts.agg.tweets$id)
  
  # join summed crimes/tweets back to data.frame
  hex@data <- left_join(hex@data, pts.agg.tweets, by = c("ZoneID" = "id"))
  hex@data <- left_join(hex@data, pts.agg.crimes, by = c("ZoneID" = "id.1"))
  
  # correlate unique zone id sums
  uniques <- hex@data[!duplicated(hex@data$num.x), ]
  cor.tc <- cor(uniques$num.x, uniques$num.y, use="complete.obs")
  
  # rbind to master frame
  cors.scale.master <- rbind(cors.scale.master, cor.tc)
}
```

\begin{figure}[!htbp]
\centering
\includegraphics{IMG/cowplot.jpeg}
\caption{Scale aggregations. A shows 10 regions, B shows 30 regions, C shows 60 regions and D shows 100 regions.}
\label{fig:scake}
\end{figure}

From \autoref{tab:scale}, we demonstrate the scaling problem. Generally, we observe rising correlations with increasing scale. Had the MAUP not existed, we would observe little systematic variation in the correlation between violent crime and tweets - we observe a difference of 0.36 between the upper and lower coefficients. Interestingly, we observe a correlation coefficient of 0.61 at 80 regions which lends evidence to @ClarkAvery findings that the low adjacency of zonal units at higher aggregations leads to lower correlation.

\begin{table}[!htbp]
\begin{center}
\caption{Correlations between crime and tweet counts.} \label{tab:scale}
\begin{tabular}{@{} *5l @{}}    \toprule
\emph{Number of regions} & \emph{Correlation coeff.} &&&  \\\midrule
 10  & 0.84  \\ 
 20  & 0.88 \\ 
 30  & 0.88 \\ 
 40  & 0.82 \\ 
 50  & 0.73 \\ 
 60  & 0.89 \\ 
 70  & 0.89 \\ 
 80  & 0.61 \\ 
 90  & 0.97 \\ 
 100 & 0.63 \\\bottomrule
 \hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{Zoning problem}

```{r, eval=FALSE}
# we choose 20 regions to illustrate zoning problem
setwd("/Users/samcomber/Documents/spatial_analysis/shp/img_stud/run1/z070")

# get all 10 iterations of 20 regions
files.20 <- list.files(pattern = "*.zon")
cors.zone.master
# build master data.frame
cors.zone.master <- data.frame()
for (file in files.20) {
  hex <- readOGR(dsn = "/Users/samcomber/Documents/spatial_analysis/shp/hex", 
                 layer = "hex_1_clip")
  
  # read file to df
  zon <- read.csv(file)
  
  # join regions to hex shapefile on area id
  hex@data$id <- as.numeric(hex@data$id)
  hex@data <- left_join(hex@data, zon, by = c("id"= "AreaID"))
  hex@data <- hex@data[order(hex@data$id),]
  
  # dissolve by area id
  tes <- unionSpatialPolygons(hex, hex@data$ZoneID)

  # convert spatial polygons to spatialpolygonsdataframe
  oldw <- getOption("warn")

  # silence irrelevant warnings
  options(warn = -1)
  df <- data.frame(id = getSpPPolygonsIDSlots(tes))
  row.names(df) <- getSpPPolygonsIDSlots(tes)
  options(warn = oldw)

  spdf <- SpatialPolygonsDataFrame(tes, data = df)

  # points in polygon spatial join
  pts.crimes <- point.in.poly(crimes, spdf)
  pts.tweets <- point.in.poly(tweets, spdf)

  # aggregate crimes/tweets data
  pts.agg.crimes <- pts.crimes@data %>% group_by(id.1) %>% summarise(num = n()) 
  %>% arrange(id.1)
  pts.agg.tweets <- pts.tweets@data %>% group_by(id) %>% summarise(num = n()) 
  %>% arrange(id)

  # cast ids as factors for left_join
  pts.agg.crimes$id.1 <- as.integer(pts.agg.crimes$id.1)
  pts.agg.tweets$id <- as.integer(pts.agg.tweets$id)

  # join summed crimes/tweets back to data.frame
  hex@data <- left_join(hex@data, pts.agg.tweets, by = c("ZoneID" = "id"))
  hex@data <- left_join(hex@data, pts.agg.crimes, by = c("ZoneID" = "id.1"))

  # correlate unique zone id sums
  uniques <- hex@data[!duplicated(hex@data$num.x), ]
  cor.tc <- cor(uniques$num.x, uniques$num.y, use="complete.obs")

  # rbind to master frame
  cors.zone.master <- rbind(cors.zone.master, cor.tc)
}
```

\begin{figure}[!htbp]
\centering
\includegraphics{IMG/cowplot_zon.jpeg}
\caption{Zone aggregations. Four iterations displayed A-D for 70 regions.}
\label{fig:zone}
\end{figure}

From \autoref{tab:zone}, we demonstrate the zoning problem: with 10 aggregation iterations for 70 regions we observe positive correlation between violent crime and tweets ranging from 0.42 (moderate assocation) to 0.96 (very strong association).

\begin{table}[!htbp]
\begin{center}
\caption{Correlation coefficients for 10 iterations of 70 regions.} \label{tab:zone}
\begin{tabular}{@{} *5l @{}}    \toprule
\emph{Iteration} & \emph{Correlation coeff.} &&&  \\\midrule
 1  & 0.89 \\ 
 2  & 0.66 \\ 
 3  & 0.61 \\ 
 4  & 0.89 \\ 
 5  & 0.42 \\ 
 6  & 0.96 \\ 
 7  & 0.75 \\ 
 8  & 0.92 \\ 
 9  & 0.82 \\ 
 10 & 0.84 \\\bottomrule
 \hline
\end{tabular}
\end{center}
\end{table}

\section{Multilevel Modelling}

Having demonstrated the MAUP, we implement a two-level multilevel model that accounts for spatial heterogeneity effects between the scale geographies. Firstly, we aggregate the median price paid per property for each LSOA:

<!-- 1) JOIN CSVS TO MAIN SHAPEFILE TO MAKE IT LOOK BETTER LOL-->

<!-- 3) RANDOM SLOPES - JUSTIFY USE WITH FACET_WRAP PLOT (WEDNESDAY) -->

<!-- 4) RESULTS (THURSDAY) -->


```{r, eval=FALSE}
# save location to tmp folder
temp <- "/tmp/lpool_pp.csv"

# download from source
download.file(paste0("http://landregistry.data.gov.uk/app/ppd/ppd_data.csv
                     ?et%5B%5D=lrcommon%3Afreehold&et%5B%5D=lrcommon%3Aleasehold
                     &header=true&limit=all&max_date=31+December+2015
                     &min_date=1+January+2015&nb%5B%5D=true&nb%5B%5D=false
                     &ptype%5B%5D=lrcommon%3Adetached&ptype%5B%5D=lrcommon%3Asemi-detached
                     &ptype%5B%5D=lrcommon%3Aterraced&ptype%5B%5D=lrcommon%
                     3Aflat-maisonette&ptype%5B%5D=lrcommon%3AotherPropertyType
                     &tc%5B%5D=ppd%3AstandardPricePaidTransaction
                     &tc%5B%5D=ppd%3AadditionalPricePaidTransaction&town=",
                     "Liverpool"), temp)

# load LR data
lr <- read.csv("/tmp/lpool_pp.csv",
               stringsAsFactors = FALSE)

# select only relevant columns
lr <- dplyr::select(lr, price_paid, property_type, new_build, saon, paon, street,
                    locality, town, postcode)

# add concatenated column for geocoding
lr.geocode <- mutate(lr, address = paste(saon, paon, street, locality, town, postcode))

# drop previous columns
lr.geocode <- dplyr::select(lr.geocode, -saon, -paon, -street, 
                            -locality, -town, -postcode)

# take sample to prevent API query limit from Google (2,500)
lr.geocode.sample <- lr.geocode[sample(nrow(lr.geocode), 2450),]

# loop over address, get lat/lon
geo.done <- data.frame()
for (i in 1:nrow(lr.geocode.sample)) {
  g <- geocode(lr.geocode.sample$address[i])
  g <- mutate(g, price=lr.geocode.sample$price_paid[i])
  geo.done <- rbind(geo.done, g)
}

# export to csv for backup
write.csv(geo.done, file = "/Users/samcomber/Documents/spatial_analysis/data/
          geocoded_lr.csv", row.names=FALSE)

# convert to spatial points df
lr <- read.csv("lpool_pp.csv", header = TRUE, sep=",")
coordinates(lr) <- ~lon + lat
proj4string(lr) <- CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")

# write once
writeOGR(lr, "/Users/samcomber/Documents/spatial_analysis/shp/spatial_join",
         "lr", driver = "ESRI Shapefile")
```

We then tidy the data, before log transforming median housing price per LSOA to relax the assumption of normality:

```{r data_prep, results="hide"}
setwd("/Users/samcomber/documents/spatial_analysis/shp/spatial_join")
# LSOA and MSOA geographies joined by location in QGIS as
# R lacks library for polygon.in.polygon joins
lsoa.join <- readOGR(dsn = ".", layer = "lsoa_join", stringsAsFactors = FALSE)
# take only dataframe
lsoa.join <- lsoa.join@data
# cast as integer
lsoa.join$md_2015 <- as.numeric(lsoa.join$md_2015)
# log housing prices
lsoa.join$md_2015 <- log(lsoa.join$md_2015)
# take only complete cases
lsoa.join <- lsoa.join[complete.cases(lsoa.join),]

# proportion over 65

```
 
\subsection{Model specification}

\subsubsection{Null Model}

Generally, our goal of estimation is the partial-pooling estimates of the median logged housing prices among all LSOAs in MSOA $j$. In our null model of no predictors, we approximate the multilevel estimate for MSOA $j$ as a weighted average of the mean of properties sold in the MSOA - an unpooled estimate $\hat{y}_j$ - and the mean across all MSOAs - the completely pooled estimate, $\hat{y}_\textnormal{all}$. As the weighted average uses information available about individual LSOAs, averages from MSOAs with a small number of constituent LSOAs carry less weighting, and so the multilevel estimate is pulled closer to Liverpool's overall average\footnote{In this way, the relative weights are determined by the sample size in the group and the variation within and between groups.} [@Gelman]. To allow this 'soft constraint' on $\alpha _j$, partial-pooling estimates are derived by assigning a probability distribution to the $\alpha_j$'s which pulls estimates of $\alpha _j$ some way towards their mean level $\mu _\alpha$ as below:

$$ \alpha_j \sim \mathcal{N}(\mu _\alpha,\,\sigma_{\alpha}^{2})\, \textnormal{for } j = 1, . . ., J, $$ 

where the mean $\mu _\alpha$ and standard deviation $\sigma _\alpha$ derived from the data.

To begin decomposing how median log housing prices vary by LSOA and MSOA geographies, we specify the following null multilevel model:

$$ y_{i,j} = \alpha_{0,j} + \epsilon_{i,j} $$

$$ \alpha_{0,j} = \alpha_0 + \mu_{0,j} $$

where in the first level, $y_{i,j}$ is the median logged housing price for the $i^{th}$ LSOA in MSOA $j$, $\alpha_{0,j}$ is the MSOA-level mean of $y_{i,j}$ (varying intercept) for the $j^{th}$ MSOA, and $\epsilon_{i,j}$ is the residual error term assumed i.i.d; where in the second level, $\alpha_0$ is the overall intercept, and finally $\mu_{0,j}$ is the random error component for the deviation of the intercept of the $j^{th}$ group from the overall intercept.

<!--  JUSTIFY NULL MODEL HERE -->

```{r null model, results="hide"}
# ----------- NULL MODEL ------------

# (1|MSOA11CD) allows intercept (coefficient of 1 is the constant 
# term in the regression) to vary by MSOA
model.null <- lmer(md_2015 ~ 1 + (1|MSOA11CD), data = lsoa.join)

# get variance
vc <- VarCorr(model.null)
print(vc,comp=c("Variance"))

# calculate variance partition coefficient
print(paste0(round(0.18341 / (0.18341 + 0.12261), 3),"%"))
```

<!--

REGRESSION TABLE HERE 

https://www.cmm.bris.ac.uk/lemma/pluginfile.php/7973/mod_resource/content/1/R-5.pdf

-->

Overall, we estimate mean attainment (across MSOAs) as *. The mean for the $j^{th}$ MSOA is estimated as $30.60 + \mu_{0,j}$, where $\mu_{0,j}$ is the MSOA-level residual\footnote{For MSOA ** DO EXAMPLE HERE}. 

Having observed a Variance Partition Coefficient (VPC) of `0.599`, we approximate ~60% of the variation in median log housing prices as explained by inequalities between MSOAs. This infers the presence of spatial heterogeneity between MSOAs, which justifies the use of a multilevel model to simultaneously capture outcomes at LSOA and MSOA geographies.


<!-- -2 log-likelihood value -->

```{r}
# catepillar graph to visualise mean of each MSOA random effect
msoa.rand.eff <- REsim(model.null)
p <- plotREsim(msoa.rand.eff)
```

\subsubsection{Random-slopes model}

To rationalise a varying intercept and slope model, we assess the variability between median housing prices and the percentage of neighbourhood income deprivation.

```{r random_slopes, eval=FALSE}
# ----------- RANDOM SLOPES ---------
sample.20 <- sample(unique(lsoa.join$MSOA11CD), 20, replace=F)

ggplot(lsoa.join[lsoa.join$MSOA11CD %in% sample.20,], aes(x = lr_income, y = md_2015)) + 
  geom_point() +
  geom_smooth(method="lm") +
  facet_wrap(~ MSOA11CD) +
  xlab("Income deprivation") +
  ylab("Median housing price") +
  theme_bw()
```

<!-- INFORM WITH DEFINITION OF ERROR TERMS FROM SLIDES -->

As \autoref{fig:vari} demonstrates variance between median logged housing prices and percentage of neighbourhood income deprivation, we are able to motivate the use of a random slope multilevel model. As observed, given random intercept models assume parallel grouping lines, they perform poorly at fitting the data. For this reason, we allow coefficients of explanatory variables to vary for each MSOA, which can be understood as interactions between group-level indicators and an individual-level predictor [@Gelman]. This is achieved by adding a random term to the coefficient of $x_k$ which relaxes the constraint of $\beta _k$ as fixed across each MSOA, allowing the coefficient to vary randomly across groups. We specify our random-slope model as follows:

$$ y_{i,j} = \alpha _{i,j} + \beta _{1,j}x_{1,ij} + \beta _{2,j}x_{2,ij} + \epsilon _{0,ij}$$

$$ \alpha _{0,j} = \alpha _{0} + \mu_{0,j} $$

$$ \beta _{1,j} = $$

which can be written as:

$$ y_{i,j} = \beta _0 + \beta $$

Conceptually, this equation is similar to the null model, but the $\mu _{1j}x_{ij}$ term is added which **. 


<!-- 

https://www.cmm.bris.ac.uk/lemma/mod/lesson/view.php?id=276
https://www.cmm.bris.ac.uk/lemma/mod/lesson/view.php?id=280

-->

```{r}
model.var.slope <- lmer(md_2015 ~ lr_income + lr_employm 
                        + (1 + lr_crime | MSOA11CD), data=lsoa.join)

# create caterpillar graph 
msoa.rand <- REsim(model.var.slope)
plotREsim(msoa.rand)
```

\subsection{Interpretation}

\section{Bibliography}
\section{Appendix}