---
title: "Assignment Two: The MAUP and Multilevel Modelling"
output:
  pdf_document:
    toc: true
    number_sections: true
bibliography: references.bib
header-includes:
   - \setlength\parindent{24pt}
---

\section{Demonstrating the MAUP}

\subsection{Background}

<!-- introduce problem -->

Areal units in zoning systems amalgamate into objects that constitute the basic units for the observation and analysis of spatial phenomena [@Openshaw]. Yet, no gold standard for guiding the spatial aggregation process exists, with the validity of zonal objects subject to the arbitrary and modifiable decision-making of quantitative geographers. Problematically, the analysis of socioeconomic data involving areal units is encumbered by the modifiable areal unit problem (MAUP): "the sensitivity of analytical results to the definition of units for which data are collected." According to the literature, the MAUP constrains the reliability of analyses for aggregated spatial data, as findings have shown varying results with the scale of aggregation and configuration of zoning systems [@ClarkAvery].

<!-- zoning problem + examples -->
<!-- aggregation problem + examples -->

In practice, the MAUP is condensed into two issues of scale and zoning sensitivity which this paper will attempt to demonstrate in Section \ref{maup}. The first issue, described as the *scale problem*, is the variation in findings when data for zonal units are progressively aggregated. This has been demonstrated empirically by @ClarkAvery who found that whilst correlation coefficients did not increase monotonically with aggregation\footnote{At higher levels of aggregation, there is smaller adjacency of zonal units meaning groupings are more heterogenous leading to lower correlation coefficients.}, a general increase in data aggregation corresponds to an increase in correlation coefficients. 

The second issue, the *zoning problem*, pertains to the variation in findings when alternative combinations of zonal units are analysed with the scale or number of units held constant [@Openshaw]. Zoning sensitivity in multivariate analysis has been demonstrated empirically in @FotheringhamWong who simulated the aggregation of 871 block groups into 218 zones in 150 different iterations. They highlight the severity of the zoning problem by demonstrating the possibility of concluding with one iteration of zones no association between the percentage of blue-collar workers and mean family income, with another iteration finding a unit increase in blue-collar worker percentages as reducing mean family income by \$20,000. In all, ignoring scale and zoning sensitivity in model calibration can lead to inferential conclusions that a researcher's areal data is applicable to the constituents who form the zones under study - the ecological fallacy problem [@Openshaw].

\subsection{MAUP analysis}
\label{maup}

\subsubsection{Data}

To demonstrate the scaling and zoning sensitivities of the MAUP, we calculate the bivariate strength of association between two open data variables. For our first variable, *crime_count*, we submit a HTTP request using the POST verb to send a custom polygon for retrieving all street-level crimes occuring in 2012. 

```{r dependencies, include=FALSE}
library(downloader)
library(rgdal)
library(httr)
```

```{r crime_data, eval=FALSE}
# download geojson
u <- "http://statistics.data.gov.uk/boundaries/E08000012.json"
# store in temporary directory 
downloader::download(url = u, destfile = "/tmp/lpool.geojson")
lpool <- readOGR(dsn = "/tmp/lpool.geojson", layer = "OGRGeoJSON")
# access coords slot
lpool <- lpool@polygons[[1]]@Polygons[[1]]@coords
# build lat/lon + date string to send with postrequest
curl.string <- paste0('poly=',paste0(sprintf('%s,%s',lpool[,2], lpool[,1])
                                     , collapse = ':'))

# build dates list for loop 
dates = c("2012-01", "2012-02", "2012-03", "2012-04", "2012-05", "2012-06",
          "2012-07", "2012-08", "2012-09", "2012-10", "2012-11", "2012-12")
dates = c("2012-01")

document <- lapply(dates, function(month) {
  # format acceptable packet for http request
  curl.string <- list(poly=c(curl.string), date=c(month))
  # post custom polygon to police api 
  r <- httr::POST("https://data.police.uk/api/crimes-street/all-crime", 
                  body = curl.string, encode="multipart", verbose())
  json <- content(r, "text", encoding = "ISO-8859-1")
  # return as data.frame
  jsonlite::fromJSON(txt=json)
})

```

Regarding our second variable, *tweet_count*, we aggregate geo-referenced tweets containing timestamps relating to Twitter postings within the municipality of Liverpool for 2012\footnote{This dataset was data mined by Guy Lansley from UCL, and processed by Dani Arribas-Bel.}.

```{r twitter_data}
tweets <- readOGR(dsn = "/Users/samcomber/Documents/spatial_analysis/shp/tweets", layer = "tweets_liverpool")
```

\subsubsection{MAUP findings}

** CORR MAP **

\section{Multilevel Modelling}

\subsection{Interpretation}

\section{Bibliography}